[
  {
    "objectID": "Homework 6.html",
    "href": "Homework 6.html",
    "title": "Homework 6",
    "section": "",
    "text": "Conceptual questions:\n\nThe purpose of the lapply() function is to apply a function to lists and obtain a list of objects that are a result of that function. The equivalent purr function to this function is the map() function.\nThe correct way to code this would be: lapply(list, cor, method = \"kendall\"). lapply will pass the third argument into the function in the second argument.\nThe main advantage of using the purr family functions is being able to use the helpers which all for you to write compact code. Additionally, the purr family of functions are type stable meaning that R will not try to guess the type of the data. This is helpful since you always know the type for the output of the purr family functions and can therefore avoid errors.\nSide-effect functions are pipeable functions that perform an action like printing something but do not change and silently return the dataframe with invisible(). This allows them to be used in piping together functions without returning the dataframe in the output. The example given in the notes is a function to count the number of rows in the dataframe. Silently returning the dataframe allows for you to use this function in piping so the input of the next function in the chain is the dataframe.\nIn a function, all of the variables are local and only exist within that function. This means that the scope of the variable name sd and the function sd are different. Within the function, the variable name sd masks the function sd but only within that function. It is due to R’s environments and lexical scoping."
  },
  {
    "objectID": "Homework 6.html#task-1",
    "href": "Homework 6.html#task-1",
    "title": "Homework 6",
    "section": "",
    "text": "Conceptual questions:\n\nThe purpose of the lapply() function is to apply a function to lists and obtain a list of objects that are a result of that function. The equivalent purr function to this function is the map() function.\nThe correct way to code this would be: lapply(list, cor, method = \"kendall\"). lapply will pass the third argument into the function in the second argument.\nThe main advantage of using the purr family functions is being able to use the helpers which all for you to write compact code. Additionally, the purr family of functions are type stable meaning that R will not try to guess the type of the data. This is helpful since you always know the type for the output of the purr family functions and can therefore avoid errors.\nSide-effect functions are pipeable functions that perform an action like printing something but do not change and silently return the dataframe with invisible(). This allows them to be used in piping together functions without returning the dataframe in the output. The example given in the notes is a function to count the number of rows in the dataframe. Silently returning the dataframe allows for you to use this function in piping so the input of the next function in the chain is the dataframe.\nIn a function, all of the variables are local and only exist within that function. This means that the scope of the variable name sd and the function sd are different. Within the function, the variable name sd masks the function sd but only within that function. It is due to R’s environments and lexical scoping."
  },
  {
    "objectID": "Homework 6.html#task-2",
    "href": "Homework 6.html#task-2",
    "title": "Homework 6",
    "section": "Task 2",
    "text": "Task 2\n\nQuestion 1\nFirst, we will write our RMSE function:\n\nget_RMSE &lt;- function(y, y_hat, ...) {\n  squared_errors &lt;- (y - y_hat)^2\n  rmse &lt;- sqrt(mean(squared_errors, ...))\n  return(rmse)\n}\n\nIn this function, the ellipsis allows for the user of the function to pass in additional arguments that will be passed into the mean function, such as na.rm = TRUE\n\n\nQuestion 2\nNow that we have the function, we can run the code from the homework.\n\nset.seed(10)\nn &lt;- 100\nx &lt;- runif(n)\nresp &lt;- 3 + 10 * x + rnorm(n)\npred &lt;- predict(lm(resp ~ x), data.frame(x))\n\nNow, lets test our function\n\nrmse &lt;- get_RMSE(resp, pred)\nrmse\n\n[1] 0.9581677\n\n\nGreat! It looks to work. Now, let’s replace two of the response values with NA.\n\nresp[5] &lt;- NA_real_\nresp[55] &lt;- NA_real_\nresp\n\n  [1]  7.674144  5.733128  8.637031 12.068788        NA  6.040709  4.843093\n  [8]  6.255948  8.512399  7.587703  8.278962  8.221201  3.304767  9.299369\n [15]  7.646876  8.504220  4.254724  5.160568  7.550652 10.115022 12.028134\n [22]  7.723097  9.702653  6.337183  5.568563 11.239175  9.903050  4.965503\n [29]  9.656077  8.081564  8.948798  3.708220  5.410925 12.714925  7.666618\n [36] 10.636295 11.886290 14.767056  8.670500  7.931076  5.338484  5.097557\n [43]  3.213884 11.444994  6.093762  3.192188  1.563749  8.753929  4.177170\n [50] 12.242498  5.781476 12.783701  4.418721  8.442989        NA  9.395394\n [57]  8.255719  6.016290  8.026494  9.180810  2.038727  5.273544  7.225220\n [64]  6.654107 12.260485 10.688362  9.773488  8.216967  5.093565  6.142304\n [71]  3.274337  8.547150  9.381826  7.061813  4.016495  7.543794  6.976389\n [78] 11.550401  5.209433  3.872522 13.043037  8.277356  3.231859  8.553664\n [85]  4.576422  2.213665 11.475262  6.469006  5.333390  5.656304  6.209727\n [92]  8.908905  6.956097  9.642321  7.188749 12.413663  6.020730  8.507994\n [99] 11.776177  3.387353\n\n\nNow that we replaced values at 5 and 55 in the response with NA. Let’s test our RMSE function with and without a na.rm argument.\n\nrmse &lt;- get_RMSE(resp, pred)\nprint(rmse)\n\n[1] NA\n\nrmse &lt;- get_RMSE(resp, pred, na.rm = TRUE)\nprint(rmse)\n\n[1] 0.9641772\n\n\nHere we can see the importance of the functionality to remove the NAs. If we do not, we will get NA as the result.\n\n\nQuestion 3\nNow, we can do the same for MAE.\n\nget_MAE &lt;- function(y, y_hat, ...){\n  absolute_error &lt;- abs(y - y_hat)\n  mae &lt;- mean(absolute_error, ...)\n  return(mae)\n}\n\n\n\nQuestion 4\nNow, we will run the code to get responses and predictions again.\n\nset.seed(10)\nn &lt;- 100\nx &lt;- runif(n)\nresp &lt;- 3 + 10 * x + rnorm(n)\npred &lt;- predict(lm(resp ~ x), data.frame(x))\n\nLet’s test in the same way we did before:\n\nmae &lt;- get_MAE(resp, pred)\nprint(mae)\n\n[1] 0.8155776\n\n# Replacing two with NA\nresp[5] &lt;- NA_real_\nresp[55] &lt;- NA_real_\nprint(resp)\n\n  [1]  7.674144  5.733128  8.637031 12.068788        NA  6.040709  4.843093\n  [8]  6.255948  8.512399  7.587703  8.278962  8.221201  3.304767  9.299369\n [15]  7.646876  8.504220  4.254724  5.160568  7.550652 10.115022 12.028134\n [22]  7.723097  9.702653  6.337183  5.568563 11.239175  9.903050  4.965503\n [29]  9.656077  8.081564  8.948798  3.708220  5.410925 12.714925  7.666618\n [36] 10.636295 11.886290 14.767056  8.670500  7.931076  5.338484  5.097557\n [43]  3.213884 11.444994  6.093762  3.192188  1.563749  8.753929  4.177170\n [50] 12.242498  5.781476 12.783701  4.418721  8.442989        NA  9.395394\n [57]  8.255719  6.016290  8.026494  9.180810  2.038727  5.273544  7.225220\n [64]  6.654107 12.260485 10.688362  9.773488  8.216967  5.093565  6.142304\n [71]  3.274337  8.547150  9.381826  7.061813  4.016495  7.543794  6.976389\n [78] 11.550401  5.209433  3.872522 13.043037  8.277356  3.231859  8.553664\n [85]  4.576422  2.213665 11.475262  6.469006  5.333390  5.656304  6.209727\n [92]  8.908905  6.956097  9.642321  7.188749 12.413663  6.020730  8.507994\n [99] 11.776177  3.387353\n\n# Test without na.rm\nmae &lt;- get_MAE(resp, pred)\nprint(mae)\n\n[1] NA\n\nmae &lt;- get_MAE(resp, pred, na.rm = TRUE)\nprint(mae)\n\n[1] 0.820124\n\n\nPerfect, everything behaved as expected.\n\n\nQuestion 5\nNow, let’s create the wrapper function described in the homework\n\nget_metrics &lt;- function(y, y_hat, metrics = c(\"RMSE\", \"MAE\"), ...) {\n  if (!(is.vector(y) && is.atomic(y) && is.numeric(y))) {\n    stop(\"`y` must be a numeric (atomic) vector.\")\n  }\n  if (!(is.vector(y_hat) && is.atomic(y_hat) && is.numeric(y_hat))) {\n    stop(\"`yhat` must be a numeric (atomic) vector.\")\n  }\n  results &lt;- list()\n  if (\"RMSE\" %in% metrics) {\n    results$RMSE &lt;- get_RMSE(y, y_hat, ...)\n  }\n  if (\"MAE\" %in% metrics) {\n    results$MAE &lt;- get_MAE(y, y_hat, ...)\n  }\n  return(results)\n}\n\n\n\nQuestion 6\nGreat, now let’s generate our data.\n\nset.seed(10)\nn &lt;- 100\nx &lt;- runif(n)\nresp &lt;- 3 + 10 * x + rnorm(n)\npred &lt;- predict(lm(resp ~ x), data.frame(x))\n\nAnd now we test!\n\nmae &lt;- get_metrics(resp, pred, \"MAE\")\nprint(mae)\n\n$MAE\n[1] 0.8155776\n\nrmse &lt;- get_metrics(resp, pred, \"RMSE\")\nprint(rmse)\n\n$RMSE\n[1] 0.9581677\n\nboth &lt;- get_metrics(resp, pred) #default behavior returns both\nprint(both)\n\n$RMSE\n[1] 0.9581677\n\n$MAE\n[1] 0.8155776\n\n\nGreat, it looks to work. Now, with NAs\n\nresp[5] &lt;- NA_real_\nresp[55] &lt;- NA_real_\nprint(resp)\n\n  [1]  7.674144  5.733128  8.637031 12.068788        NA  6.040709  4.843093\n  [8]  6.255948  8.512399  7.587703  8.278962  8.221201  3.304767  9.299369\n [15]  7.646876  8.504220  4.254724  5.160568  7.550652 10.115022 12.028134\n [22]  7.723097  9.702653  6.337183  5.568563 11.239175  9.903050  4.965503\n [29]  9.656077  8.081564  8.948798  3.708220  5.410925 12.714925  7.666618\n [36] 10.636295 11.886290 14.767056  8.670500  7.931076  5.338484  5.097557\n [43]  3.213884 11.444994  6.093762  3.192188  1.563749  8.753929  4.177170\n [50] 12.242498  5.781476 12.783701  4.418721  8.442989        NA  9.395394\n [57]  8.255719  6.016290  8.026494  9.180810  2.038727  5.273544  7.225220\n [64]  6.654107 12.260485 10.688362  9.773488  8.216967  5.093565  6.142304\n [71]  3.274337  8.547150  9.381826  7.061813  4.016495  7.543794  6.976389\n [78] 11.550401  5.209433  3.872522 13.043037  8.277356  3.231859  8.553664\n [85]  4.576422  2.213665 11.475262  6.469006  5.333390  5.656304  6.209727\n [92]  8.908905  6.956097  9.642321  7.188749 12.413663  6.020730  8.507994\n [99] 11.776177  3.387353\n\nmae &lt;- get_metrics(resp, pred, \"MAE\")\nprint(mae)\n\n$MAE\n[1] NA\n\nmae &lt;- get_metrics(resp, pred, \"MAE\", na.rm = TRUE)\nprint(mae)\n\n$MAE\n[1] 0.820124\n\nrmse &lt;- get_metrics(resp, pred, \"RMSE\")\nprint(rmse)\n\n$RMSE\n[1] NA\n\nrmse &lt;- get_metrics(resp, pred, \"RMSE\", na.rm = TRUE)\nprint(rmse)\n\n$RMSE\n[1] 0.9641772\n\nboth &lt;- get_metrics(resp, pred) #default behavior returns both\nprint(both)\n\n$RMSE\n[1] NA\n\n$MAE\n[1] NA\n\nboth &lt;- get_metrics(resp, pred, na.rm = TRUE) #default behavior returns both\nprint(both)\n\n$RMSE\n[1] 0.9641772\n\n$MAE\n[1] 0.820124\n\n\nAgain, everything works as expected. Now, what if we replace our data with a dataframe.\n\nerror &lt;- get_metrics(iris, iris)\n\nError in get_metrics(iris, iris): `y` must be a numeric (atomic) vector.\n\n\nPerfect, we get the error we expected."
  },
  {
    "objectID": "Homework 6.html#task-3",
    "href": "Homework 6.html#task-3",
    "title": "Homework 6",
    "section": "Task 3",
    "text": "Task 3\nLet’s create the list object first. We will also load in tidyverse.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlm_fit1 &lt;- lm(Sepal.Length ~ Sepal.Width + Species, data = iris)\n\n\nQuestion 1\nLet’s pull out the coefficients using the three methods described in the homework\n\n# First method\ncoef1 &lt;- lm_fit1$coefficients\nprint(coef1)\n\n      (Intercept)       Sepal.Width Speciesversicolor  Speciesvirginica \n        2.2513932         0.8035609         1.4587431         1.9468166 \n\ncoef2 &lt;- coef(lm_fit1)\nprint(coef2)\n\n      (Intercept)       Sepal.Width Speciesversicolor  Speciesvirginica \n        2.2513932         0.8035609         1.4587431         1.9468166 \n\ncoef3 &lt;- lm_fit1 |&gt; pluck(\"coefficients\")\nprint(coef3)\n\n      (Intercept)       Sepal.Width Speciesversicolor  Speciesvirginica \n        2.2513932         0.8035609         1.4587431         1.9468166 \n\n\nAll three methods return the same coefficients as expected.\n\n\nQuestion 2\nFirst, let’s fit our models.\n\nlm_fit2 &lt;- lm(Sepal.Length ~ Sepal.Width, data = iris)\nlm_fit3 &lt;- lm(Sepal.Length ~ Petal.Width + Sepal.Width + Species, data = iris)\nlm_fit4 &lt;- lm(Sepal.Length ~ Petal.Width + Petal.Length + Sepal.Width + Species,\ndata = iris)\nfits &lt;- list(lm_fit1, lm_fit2, lm_fit3, lm_fit4)\n\nNow we can use a combo of pluck and map to pluck all of the coefficients from fits.\n\nfits |&gt; map(pluck(\"coefficients\"))\n\n[[1]]\n      (Intercept)       Sepal.Width Speciesversicolor  Speciesvirginica \n        2.2513932         0.8035609         1.4587431         1.9468166 \n\n[[2]]\n(Intercept) Sepal.Width \n  6.5262226  -0.2233611 \n\n[[3]]\n      (Intercept)       Petal.Width       Sepal.Width Speciesversicolor \n        2.5210733         0.3715768         0.6982260         0.9881297 \n Speciesvirginica \n        1.2375878 \n\n[[4]]\n      (Intercept)       Petal.Width      Petal.Length       Sepal.Width \n        2.1712663        -0.3151552         0.8292439         0.4958889 \nSpeciesversicolor  Speciesvirginica \n       -0.7235620        -1.0234978 \n\n\nPerfect, it works as expected.\n\n\nQuestion 3\nNow, we can us map to get confidence intervals as well.\n\nfits |&gt; map(confint)\n\n[[1]]\n                      2.5 %   97.5 %\n(Intercept)       1.5206309 2.982156\nSepal.Width       0.5933983 1.013723\nSpeciesversicolor 1.2371791 1.680307\nSpeciesvirginica  1.7491525 2.144481\n\n[[2]]\n                2.5 %     97.5 %\n(Intercept)  5.579865 7.47258038\nSepal.Width -0.529820 0.08309785\n\n[[3]]\n                        2.5 %    97.5 %\n(Intercept)        1.74261803 3.2995285\nPetal.Width       -0.02042746 0.7635811\nSepal.Width        0.46205710 0.9343950\nSpeciesversicolor  0.44520784 1.5310516\nSpeciesvirginica   0.46412393 2.0110518\n\n[[4]]\n                       2.5 %      97.5 %\n(Intercept)        1.6182321  2.72430044\nPetal.Width       -0.6140049 -0.01630542\nPetal.Length       0.6937939  0.96469395\nSepal.Width        0.3257653  0.66601260\nSpeciesversicolor -1.1982739 -0.24885002\nSpeciesvirginica  -1.6831329 -0.36386273\n\n\nGreat! Looks to have worked.\n\n\nQuestion 4\nNow, let’s aim to plot the residuals.\n\npar(mfrow = c(2, 2))\nfits |&gt; map(residuals) |&gt; walk(hist)\n\n\n\n\n\n\n\n\nThe plots look great! But the names are not too great.\n\n\nQuestion 5\nLet’s try to fix that!\n\npar(mfrow = c(2, 2))\nfits |&gt; map(residuals) |&gt; set_names(c(\"fit1\", \"fit2\", \"fit3\", \"fit4\")) |&gt; iwalk(\\(x, y) hist(x, main = y))"
  }
]